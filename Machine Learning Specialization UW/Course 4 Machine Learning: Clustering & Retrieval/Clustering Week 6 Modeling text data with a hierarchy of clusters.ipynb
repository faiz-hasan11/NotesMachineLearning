{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans                # we'll be using scikit-learn's KMeans for this assignment\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we extract the TF-IDF vector of each document.\n",
    "\n",
    "For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.\n",
    "\n",
    "To load in the TF-IDF vectors, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "\n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')\n",
    "map_index_to_word = json.loads(open('people_wiki_map_index_to_word.json').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with the k-means assignment, let's normalize all vectors to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartition the Wikipedia dataset using k-means\n",
    "\n",
    "Recall our workflow for clustering text data with k-means:\n",
    "\n",
    "  *  Load the dataframe containing a dataset, such as the Wikipedia text dataset.\n",
    "  *  Extract the data matrix from the dataframe.\n",
    "  *  Run k-means on the data matrix with some value of k.\n",
    "  *  Visualize the clustering results using the centroids, cluster assignments, and the original dataframe. We keep the original dataframe around because the data matrix does not keep auxiliary information (in the case of the text dataset, the title of each article).\n",
    "\n",
    "Let us modify the workflow to perform bipartitioning:\n",
    "\n",
    "  *  Load the dataframe containing a dataset, such as the Wikipedia text dataset.\n",
    "  *  Extract the data matrix from the dataframe.\n",
    "  *  Run k-means on the data matrix with k=2.\n",
    "  *  Divide the data matrix into two parts using the cluster assignments.\n",
    "  *  Divide the dataframe into two parts, again using the cluster assignments. This step is necessary to allow for visualization.\n",
    "  *  Visualize the bipartition of data.\n",
    "\n",
    "We'd like to be able to repeat Steps 3-6 multiple times to produce a hierarchy of clusters such as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                      (root)\n",
    "                         |\n",
    "            +------------+-------------+\n",
    "            |                          |\n",
    "         Cluster                    Cluster\n",
    "     +------+-----+             +------+-----+\n",
    "     |            |             |            |\n",
    "    Cluster     Cluster       Cluster      Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parent cluster is bipartitioned to produce two child clusters. At the very top is the root cluster, which consists of the entire dataset.\n",
    "\n",
    "Now we write a wrapper function to bipartition a given cluster using k-means. There are three variables that together comprise the cluster:\n",
    "\n",
    " *   dataframe: a subset of the original dataframe that correspond to member rows of the cluster\n",
    " *   matrix: same set of rows, stored in sparse matrix format\n",
    " *   centroid: the centroid of the cluster (not applicable for the root cluster)\n",
    "\n",
    "Rather than passing around the three variables separately, we package them into a Python dictionary. The wrapper function takes a single dictionary (representing a parent cluster) and returns two dictionaries (representing the child clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bipartition(cluster, maxiter=400, num_runs=4, seed=None):\n",
    "    '''cluster: should be a dictionary containing the following keys\n",
    "                * dataframe: original dataframe\n",
    "                * matrix:    same data, in matrix format\n",
    "                * centroid:  centroid for this particular cluster'''\n",
    "    \n",
    "    data_matrix = cluster['matrix']\n",
    "    dataframe   = cluster['dataframe']\n",
    "    \n",
    "    # Run k-means on the data matrix with k=2. We use scikit-learn here to simplify workflow.\n",
    "    kmeans_model = KMeans(n_clusters=2, max_iter=maxiter, n_init=num_runs, random_state=seed, n_jobs=1)    \n",
    "    kmeans_model.fit(data_matrix)\n",
    "    centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "    \n",
    "    # Divide the data matrix into two parts using the cluster assignments.\n",
    "    data_matrix_left_child, data_matrix_right_child = data_matrix[cluster_assignment==0], \\\n",
    "                                                      data_matrix[cluster_assignment==1]\n",
    "    \n",
    "    # Divide the dataframe into two parts, again using the cluster assignments.\n",
    "    cluster_assignment_sa = sframe.SArray(cluster_assignment) # minor format conversion\n",
    "    dataframe_left_child, dataframe_right_child     = dataframe[cluster_assignment_sa==0], \\\n",
    "                                                      dataframe[cluster_assignment_sa==1]\n",
    "        \n",
    "    \n",
    "    # Package relevant variables for the child clusters\n",
    "    cluster_left_child  = {'matrix': data_matrix_left_child,\n",
    "                           'dataframe': dataframe_left_child,\n",
    "                           'centroid': centroids[0]}\n",
    "    cluster_right_child = {'matrix': data_matrix_right_child,\n",
    "                           'dataframe': dataframe_right_child,\n",
    "                           'centroid': centroids[1]}\n",
    "    \n",
    "    return (cluster_left_child, cluster_right_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs bipartitioning of the Wikipedia dataset. Allow 20-60 seconds to finish.\n",
    "\n",
    "Note. For the purpose of the assignment, we set an explicit seed (seed=1) to produce identical outputs for every run. In pratical applications, you might want to use different random seeds for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_data = {'matrix': tf_idf, 'dataframe': wiki} # no 'centroid' for the root cluster\n",
    "left_child, right_child = bipartition(wiki_data, maxiter=100, num_runs=6, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the contents of one of the two clusters, which we call the left_child, referring to the tree visualization above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(left_child)\n",
    "print(right_child)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
